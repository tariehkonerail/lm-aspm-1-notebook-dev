{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# This notebook gives a demo to load the binary classifier models built for predicting the success of an LP Attempt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "### Loading the binary_models.yaml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import truncate\n",
    "import sys\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "import os\n",
    "import yaml\n",
    "import h3\n",
    "\n",
    "path_root = Path(os.path.abspath('')).parents[0] / 'lib'\n",
    "sys.path.append(str(path_root))\n",
    "from ds_shared.service_level import unify_service_levels, lp_sla_time_in_minutes\n",
    "\n",
    "# Define the path where models are stored\n",
    "MODEL_FOLDER = Path().resolve().parents[0] / 'output_models'\n",
    "\n",
    "# Define the file path\n",
    "yaml_file_path = MODEL_FOLDER / \"binary_models.yaml\"\n",
    "\n",
    "# Read from a YAML file\n",
    "with open(yaml_file_path, 'r') as file:\n",
    "    yaml_data = yaml.safe_load(file)\n",
    "\n",
    "best_resolution = yaml_data['geospatial_resolution']\n",
    "\n",
    "predictors = {}\n",
    "for model_num in range(4):\n",
    "    predictors[model_num] =  yaml_data['feature_sets'][f'set{model_num}']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Loading all the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of models\n",
    "n_models = len(predictors)\n",
    "all_models = {}\n",
    "\n",
    "# Load the models\n",
    "for i in range(n_models):\n",
    "    fname = 'model_set' + str(i) + '.joblib'\n",
    "    all_models[i] = joblib.load(MODEL_FOLDER / fname)\n",
    "    print(f\"Loaded models from: {MODEL_FOLDER / fname}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "# Loading the past month's data to update driver features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(\"Extracting data!!\")\n",
    "!{sys.executable} ../data_extraction/sf-extract-attempt-data-evenly-by-sla.py --capture_days 30 --folder_name 'past_month' --sample_rate 10\n",
    "\n",
    "print(\"Combining CSVs!!\")\n",
    "!{sys.executable} ../data_extraction/combine_csvs.py --folder_name 'past_month'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the pricing data\n",
    "DATA = Path().resolve().parents[0] / 'past_month'\n",
    "PRICING_SET = DATA / 'attempt_success_prediction_final_mi.csv'\n",
    "df_month = pd.read_csv(PRICING_SET, dtype={7: str})\n",
    "df_month = df_month.sort_values(by='dispatchedOn').reset_index(drop=True)\n",
    "df_month = df_month.drop_duplicates(subset=['attemptId'], keep=False)\n",
    "\n",
    "df_month['attemptSuccess'] = df_month.apply(lambda x: 1 if x.attemptStatus == 'DELIVERED' else 0, axis=1)\n",
    "df_month['dispatchedOn'] = pd.to_datetime(df_month['dispatchedOn'])\n",
    "\n",
    "df_month, all_sls = unify_service_levels(df_month)\n",
    "# Including only the rows with serviceLevels in the desired modes\n",
    "df_month = df_month[df_month['serviceLevel'].isin(lp_sla_time_in_minutes.keys())]\n",
    "\n",
    "# Adding the geospatial feature\n",
    "df_month['h3_zone'] = df_month.apply(lambda row: h3.latlng_to_cell(row.fromLat, row.fromLon, best_resolution), axis=1).astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "past_1mo_stats = df_month.groupby('lpOrganizationId').agg(\n",
    "    past_1mo_attempts=('attemptSuccess', 'size'),\n",
    "    past_1mo_success=('attemptSuccess', 'sum')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_month.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "# Testing on sample raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample test data\n",
    "path_root = Path(os.path.abspath('')).parents[0] / 'lib'\n",
    "sys.path.append(str(path_root))\n",
    "from ds_shared.service_level import unify_service_levels\n",
    "\n",
    "# Create a new row as a dictionary\n",
    "raw_data = {'distanceMi': 50, 'weightLbsTotal': 0, 'largestDimIn': 0, 'secondDimIn': 0, 'thirdDimIn': 0, 'largestWeightLbs': 0, 'serviceLevel': '1 Minute', 'fromLat': 32, 'fromLon': -90, 'lpOrganizationId':'16b6c714-f8d7-4a81-9ac2-a3938f3450c2'}\n",
    "\n",
    "X_test = pd.DataFrame([raw_data])\n",
    "X_test, all_sls = unify_service_levels(X_test)\n",
    "\n",
    "#X_test['serviceLevel'].isin(lp_sla_time_in_minutes.keys())\n",
    "\n",
    "X_test['h3_zone'] = X_test.apply(lambda x: h3.latlng_to_cell(x.fromLat, x.fromLon, best_resolution), axis=1).astype('category')\n",
    "X_test['past_1mo_total_attempts'] = X_test.apply(lambda x: past_1mo_stats.past_1mo_attempts.get(x.lpOrganizationId, 0), axis=1)\n",
    "X_test['past_1mo_successes'] = X_test.apply(lambda x: past_1mo_stats.past_1mo_success.get(x.lpOrganizationId, 0), axis=1)\n",
    "\n",
    "\n",
    "# This is the check for a single instance, do modify when you have multiple rows\n",
    "if(X_test['serviceLevel'].isin(lp_sla_time_in_minutes.keys()).loc[0]):\n",
    "    if(('secondDimIn' in X_test.columns) and ('thirdDimIn' in X_test.columns)):\n",
    "        model_num = 3\n",
    "    else:\n",
    "        model_num = 1\n",
    "else:\n",
    "    if(('secondDimIn' in X_test.columns) and ('thirdDimIn' in X_test.columns)):\n",
    "        model_num = 2\n",
    "    else:\n",
    "        model_num = 0\n",
    "\n",
    "print('For teh above set of features using model#{} and its prediction is {}'.format(model_num, all_models[model_num].predict(X_test[predictors[model_num]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
